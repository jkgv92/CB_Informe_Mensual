{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# BC 424 - Honda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "DEVICE_NAME = 'BC 424 - Honda'\n",
    "PICKLED_DATA_FILENAME = 'data_monthly.pkl'\n",
    "project_path = 'D:\\Bancolombia\\CB_Informe_Mensual'\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import json\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "import seaborn as sns\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from dotenv import dotenv_values\n",
    "\n",
    "from sys import exit\n",
    "import os\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_path = Path(project_path)\n",
    "sys.path.append(str(project_path))\n",
    "\n",
    "import config as cfg\n",
    "from library_ubidots import Ubidots\n",
    "\n",
    "\n",
    "_TOKEN = os.getenv(\"_token\")\n",
    "PICKLED_DATA_FILEPATH = project_path / 'data' / PICKLED_DATA_FILENAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "def calculate_interval_duration_days(interval):\n",
    "    dt_start = pd.to_datetime(interval['start'])\n",
    "    dt_end = pd.to_datetime(interval['end'])\n",
    "    return (dt_end - dt_start).days\n",
    "\n",
    "\n",
    "def count_days(df):\n",
    "    return len(np.unique(df.index.date))\n",
    "\n",
    "\n",
    "def find_date_overlap(baseline, study):\n",
    "    latest_start = max(\n",
    "        pd.to_datetime(baseline['start']),\n",
    "        pd.to_datetime(study['start'])\n",
    "    )\n",
    "    earliest_end = min(\n",
    "        pd.to_datetime(baseline['end']),\n",
    "        pd.to_datetime(study['end'])\n",
    "    )\n",
    "    delta = (earliest_end - latest_start).days + 1\n",
    "    return int(max(0, delta))\n",
    "\n",
    "\n",
    "def check_intervals(baseline, study, max_overlap=0):\n",
    "    overlap = find_date_overlap(baseline, study)\n",
    "    if (overlap > max_overlap):\n",
    "        print(\n",
    "            \"Error: The baseline and study intervals must not overlap. \"\n",
    "            f\"Current overlap is {overlap} days\"\n",
    "        )\n",
    "        exit()\n",
    "\n",
    "\n",
    "def make_request(LST_DEVICE_ID_TO_REQUEST, LST_VAR_LABELS, LST_VAR_FIELDS, date_interval, DATE_FORMAT, _TOKEN):\n",
    "    # LST_DEVICE_ID_TO_REQUEST must be a list of devices, even if it contains only one item\n",
    "    if not isinstance(LST_DEVICE_ID_TO_REQUEST, list):\n",
    "        LST_DEVICE_ID_TO_REQUEST = [LST_DEVICE_ID_TO_REQUEST]\n",
    "\n",
    "    # For bulk raw data requests the Ubidots API requires\n",
    "    # a list of variable IDs, not device IDs.\n",
    "    # Starting from variable labels requires fetching all \n",
    "    # variable info, then filtering IDs based on labels.\n",
    "    DCT_AVAILABLE_VAR_ID = Ubidots.get_var_id_for_multiple_devices(LST_DEVICE_ID_TO_REQUEST, LST_VAR_LABELS, _TOKEN)\n",
    "\n",
    "    # the request must be made in millisecond timestamps\n",
    "    start_timestamp = str_date_to_int_timestamp_ms(date_interval['start'], DATE_FORMAT)\n",
    "    end_timestamp = str_date_to_int_timestamp_ms(date_interval['end'], DATE_FORMAT)\n",
    "\n",
    "    # The request is made with an array of all variable IDs.\n",
    "    response = Ubidots.get_raw_data(\n",
    "        list(DCT_AVAILABLE_VAR_ID.keys()), \n",
    "        LST_VAR_FIELDS, \n",
    "        start_timestamp, \n",
    "        end_timestamp, \n",
    "        _TOKEN, \n",
    "        join=False\n",
    "    )\n",
    "\n",
    "    # The connection is left open by default\n",
    "    response.close()\n",
    "    return response\n",
    "\n",
    "def str_date_to_int_timestamp_ms(date_string, date_format):\n",
    "    element = datetime.strptime(date_string, date_format)\n",
    "    return int(datetime.timestamp(element)) * 1000\n",
    "\n",
    "def flatten_bulk_raw_response(r_json_data, headers):\n",
    "    lst_df_idx = []\n",
    "    for idx in range(len(r_json_data)):\n",
    "        df_idx = pd.DataFrame(r_json_data[idx], columns=headers)\n",
    "        lst_df_idx.append(df_idx)\n",
    "\n",
    "    return pd.concat(lst_df_idx).reset_index(drop=True)\n",
    "\n",
    "def convert_timezone(obj, from_tz='utc', to_tz='America/Bogota'):\n",
    "    if isinstance(obj, str):\n",
    "        obj = pd.to_datetime(obj).tz_localize(from_tz).tz_convert(to_tz)\n",
    "    elif isinstance(obj, datetime):\n",
    "        obj.tz_localize(from_tz).tz_convert(to_tz)\n",
    "    elif isinstance(obj, pd.DataFrame):\n",
    "        # A DatetimeIndex must be set to allow for easy \n",
    "        # timezone conversion\n",
    "        obj.set_index('datetime', inplace=True)\n",
    "        obj = obj.tz_localize(from_tz).tz_convert(to_tz)\n",
    "\n",
    "    return obj\n",
    "\n",
    "def parse_flat_data(df, DCT_AVAILABLE_VAR_ID):\n",
    "    # The Ubidots API does not return a variable-label field\n",
    "    # and naming is inconsistent, so labels must be mapped from ids.\n",
    "    df['variable'] = df['variable'].map(DCT_AVAILABLE_VAR_ID)\n",
    "\n",
    "    # datetimes are human readable\n",
    "    df[\"datetime\"] = pd.to_datetime(df[\"timestamp\"], unit='ms')\n",
    "    df = convert_timezone(df)\n",
    "    \n",
    "    df.drop_duplicates(subset=['timestamp', 'variable', 'device'], inplace=True)\n",
    "    return df.drop(columns='timestamp')\n",
    "\n",
    "def get_available_devices(device_group_label):\n",
    "    # The API requires a tilde leading the device group label\n",
    "    # but the user shouldn't be expected to know this\n",
    "    tilde_device_group_label = '~' + device_group_label\n",
    "    r_devices = Ubidots.get_device_group_devices(_TOKEN, tilde_device_group_label)\n",
    "    dct_available_devices = dict(zip(r_devices['id'], r_devices['label']))\n",
    "    return dct_available_devices\n",
    "\n",
    "def get_available_variables(device_id):\n",
    "    if isinstance(device_id, list):\n",
    "        dct_variables = Ubidots.get_var_id_for_multiple_devices(device_id, LST_VAR_LABELS, _TOKEN)\n",
    "    else:\n",
    "        variables = Ubidots.get_all_variables_from_device(_TOKEN, device_id)\n",
    "        dct_variables = dict(zip(variables['variable_id'], variables['variable_label']))\n",
    "    return dct_variables\n",
    "\n",
    "def show_available_devices(dct_available_devices):\n",
    "    print(\"Available devices in group:\")\n",
    "    print(json.dumps(dct_available_devices, sort_keys=True, indent=4))\n",
    "    return dct_available_devices\n",
    "\n",
    "def show_available_variables(dct_variables):\n",
    "    print(\"Available variables:\")\n",
    "    print(json.dumps(dct_variables, sort_keys=True, indent=4))\n",
    "    return dct_variables\n",
    "\n",
    "def show_response_contents(df):\n",
    "    print(\"The response contains:\")\n",
    "    print(json.dumps(list(df['variable'].unique()), sort_keys=True, indent=4))\n",
    "    print(json.dumps(list(df['device'].unique()), sort_keys=True, indent=4))\n",
    "\n",
    "def show_variable_labels_to_request():\n",
    "    print(\"Variable labels to request: \")\n",
    "    print(json.dumps(LST_VAR_LABELS, sort_keys=True, indent=4))\n",
    "\n",
    "def show_outlier_counts(df):\n",
    "    is_selection = (\n",
    "        (df['outlier']==True)\n",
    "    )\n",
    "    print(\"Outlier counts:\")\n",
    "    print(df.loc[is_selection, 'variable'].value_counts())\n",
    "\n",
    "def double_subset_rolling_clean(df, subset_1=None, subset_2=None, clean_on=None):\n",
    "    lst_df = []\n",
    "    for ss_label_1 in df[subset_1].unique():\n",
    "        for ss_label_2 in df[subset_2].unique():\n",
    "            is_selection = (\n",
    "                (df[subset_1]==ss_label_1)\n",
    "                & (df[subset_2]==ss_label_2)\n",
    "            )\n",
    "            df_temp = df[is_selection].copy()\n",
    "            series = df_temp[clean_on]\n",
    "\n",
    "            # long window high percentile\n",
    "            is_long_outlier = rolling_percentile_outlier(\n",
    "                series, \n",
    "                LONG_WINDOW, \n",
    "                LONG_CONFIDENCE_INTERVAL\n",
    "            )\n",
    "\n",
    "            # TODO: find a way to filter out long outliers\n",
    "            # before feeding into short outlier method\n",
    "\n",
    "            # short window low percentile\n",
    "            is_short_outlier = rolling_percentile_outlier(\n",
    "                series, \n",
    "                SHORT_WINDOW, \n",
    "                SHORT_CONFIDENCE_INTERVAL\n",
    "            )\n",
    "\n",
    "            if (SMOOTHING_METHOD == 'median'):\n",
    "                df_temp[clean_on] = series.rolling(window=SMOOTHING_WINDOW, center=True).median()\n",
    "            elif (SMOOTHING_METHOD == 'mean'):\n",
    "                df_temp[clean_on] = series.rolling(window=SMOOTHING_WINDOW, center=True).mean()\n",
    "\n",
    "            df_temp['outlier'] = ((is_long_outlier) | (is_short_outlier))\n",
    "            lst_df.append(df_temp)\n",
    "\n",
    "    return pd.concat(lst_df)\n",
    "\n",
    "def rolling_percentile_outlier(series, window, confidence_interval):\n",
    "    upper_quantile = (1 + confidence_interval/100)/2\n",
    "    lower_quantile = 1 - upper_quantile\n",
    "    \n",
    "    s_upper_percentile = series.rolling(window=window, center=True).quantile(\n",
    "        quantile=upper_quantile, \n",
    "        axis=0,\n",
    "        numeric_only=True, \n",
    "        interpolation='linear'\n",
    "    )\n",
    "\n",
    "    s_lower_percentile = series.rolling(window=window, center=True).quantile(\n",
    "        quantile=lower_quantile, \n",
    "        axis=0,\n",
    "        numeric_only=True, \n",
    "        interpolation='linear'\n",
    "    )\n",
    "\n",
    "    is_outlier = (\n",
    "        (series > s_upper_percentile)\n",
    "        | (series < s_lower_percentile)\n",
    "    )\n",
    "\n",
    "    return is_outlier\n",
    "\n",
    "def discard_date_intervals(df):\n",
    "    for interval in DATE_INTERVALS_TO_DISCARD:\n",
    "        is_outside_range = (\n",
    "            (df.index < interval[0])\n",
    "            | (df.index > interval[1])\n",
    "        )\n",
    "    return df[is_outside_range].copy()\n",
    "\n",
    "def run_cleaning_analysis(variable=None, start_date=None, end_date=None, bins=None, wide_figsize=(30,10), square_figsize=(10,10)):\n",
    "    device_name = df['device_name'][0]\n",
    "\n",
    "    is_sel_1 = (\n",
    "        (df['variable']==variable)\n",
    "        & (df.index>start_date)\n",
    "        & (df.index<end_date)\n",
    "    )\n",
    "\n",
    "    is_sel_2 = (\n",
    "        (df['outlier']==False)\n",
    "        & (df['variable']==variable)\n",
    "        & (df.index>start_date)\n",
    "        & (df.index<end_date)\n",
    "    )\n",
    "\n",
    "    s_1 = df.loc[is_sel_1, 'value']\n",
    "    s_2 = df.loc[is_sel_2, 'value']\n",
    "\n",
    "    s_res = s_1 - s_2\n",
    "    lst_series = [\n",
    "        s_1, \n",
    "        s_2\n",
    "    ]\n",
    "\n",
    "    plot_list_series(lst_series, device_name, wide_figsize, draw_markers=False)\n",
    "    plot_list_series([s_res], device_name, wide_figsize, draw_markers=False)\n",
    "\n",
    "    plt.figure(figsize=square_figsize)\n",
    "    plt.scatter(s_res.index, s_res)\n",
    "    plt.show()\n",
    "\n",
    "    s_res.hist(bins=bins, figsize=square_figsize)\n",
    "\n",
    "def request_data():\n",
    "    # A user might select baseline and study intervals\n",
    "    # which are sufficiently far apart that fetching\n",
    "    # the data in between is very inefficient.\n",
    "    # So it's best to make a request per interval.\n",
    "    response_bl = make_request(\n",
    "        LST_DEVICE_ID_TO_REQUEST, \n",
    "        LST_VAR_LABELS, \n",
    "        LST_VAR_FIELDS, \n",
    "        BASELINE_DATE_INTERVAL, \n",
    "        DATE_FORMAT, \n",
    "        _TOKEN\n",
    "    )\n",
    "\n",
    "    response_st = make_request(\n",
    "        LST_DEVICE_ID_TO_REQUEST, \n",
    "        LST_VAR_LABELS, \n",
    "        LST_VAR_FIELDS, \n",
    "        STUDY_DATE_INTERVAL, \n",
    "        DATE_FORMAT, \n",
    "        _TOKEN\n",
    "    )\n",
    "    return response_bl, response_st\n",
    "\n",
    "\n",
    "def parse_response(response_bl, response_st):\n",
    "    # The response is a JSON and it must be flattened into a table\n",
    "    df_bl = flatten_bulk_raw_response(response_bl.json()['results'], LST_HEADERS)\n",
    "\n",
    "    # Some parsing is required, including pivoting by variable\n",
    "    df_bl = parse_flat_data(df_bl, DCT_AVAILABLE_VARIABLES)\n",
    "\n",
    "    # The response is a JSON and it must be flattened into a table\n",
    "    df_st = flatten_bulk_raw_response(response_st.json()['results'], LST_HEADERS)\n",
    "\n",
    "    # Some parsing is required, including pivoting by variable\n",
    "    df_st = parse_flat_data(df_st, DCT_AVAILABLE_VARIABLES)\n",
    "\n",
    "    print(f\"Shape of baseline data: {df_bl.shape}\")\n",
    "    print(f\"Shape of study data: {df_st.shape}\")\n",
    "\n",
    "    return pd.concat([df_bl, df_st])\n",
    "\n",
    "def apply_datetime_transformations(df):\n",
    "    df['dow'] = pd.to_datetime(df.index).dayofweek\n",
    "    df['dow'] = df['dow'].map(dct_dow)\n",
    "    \n",
    "    df['hour'] = pd.to_datetime(df.index).hour\n",
    "    df['year'] = pd.to_datetime(df.index).year\n",
    "    df['month'] = pd.to_datetime(df.index).month\n",
    "    df['day'] = pd.to_datetime(df.index).day\n",
    "\n",
    "    df.sort_values(by=['datetime', 'variable'], inplace=True)\n",
    "    return df\n",
    "\n",
    "def post_process_data(df):\n",
    "    # Discard first entry before cleaning as it might not belong to interval\n",
    "    # sort by datetime to guarantee chronological order when dropping rows\n",
    "    df.sort_values(by=['datetime', 'variable'], inplace=True)\n",
    "    # TODO: convert to double subset on devices and variables.\n",
    "    # n-vars must be dropped per variable per device\n",
    "    df = subset_drop_first_n_rows(df, subset='variable', n_rows=1)\n",
    "\n",
    "    # TODO: organize this cell better. Maybe wrap in functions.\n",
    "    if len(DATE_INTERVALS_TO_DISCARD)>0:\n",
    "        df = discard_date_intervals(df, DATE_INTERVALS_TO_DISCARD)\n",
    "\n",
    "    if (clean_data is True):\n",
    "        df = double_subset_rolling_clean(\n",
    "            df, \n",
    "            subset_1='device', \n",
    "            subset_2='variable', \n",
    "            clean_on='value'\n",
    "        )\n",
    "\n",
    "    # plotting requires day of week and hour of day labels\n",
    "\n",
    "\n",
    "    apply_datetime_transformations(df)\n",
    "\n",
    "\n",
    "    return df\n",
    "\n",
    "def split_into_baseline_and_study(df, baseline_interval, study_interval):\n",
    "    # Slicing on non-monotonic indexes is deprecated so sorting is a must\n",
    "    df.sort_index(inplace=True)\n",
    "    df_baseline = df[baseline_interval['start']:baseline_interval['end']]\n",
    "    df_study = df[study_interval['start']:study_interval['end']]\n",
    "    return df_baseline, df_study\n",
    "\n",
    "def discard_date_intervals(df, discard_date_interval):\n",
    "    for interval in discard_date_interval:\n",
    "        is_outside_range = (\n",
    "            (df.index < interval[0])\n",
    "            | (df.index > interval[1])\n",
    "        )\n",
    "    return df[is_outside_range].copy()\n",
    "\n",
    "def plot_list_series(lst_series, device_name, figsize, draw_markers=True):\n",
    "    plt.figure(figsize=figsize)\n",
    "    for series in lst_series:\n",
    "        plt.plot(series, linestyle='-', alpha=0.8)\n",
    "\n",
    "        if (draw_markers is True):\n",
    "            plt.scatter(series.index, series, s=2, alpha=1, color='k')\n",
    "        \n",
    "    plt.title(f\"{device_name}\")\n",
    "    plt.ylim(bottom=-1)\n",
    "    plt.axhline(y=0, color='k')\n",
    "    plt.show()\n",
    "\n",
    "def subset_plot_long_df(df, device_label, wide_figure_size, values=None, subset=None, draw_markers=False):\n",
    "    lst_series = []\n",
    "    for subset_var in df[subset].unique():\n",
    "        series = df.loc[(df[subset]==subset_var), values]\n",
    "        lst_series.append(series.rename(subset_var))\n",
    "\n",
    "    # print(lst_series)\n",
    "\n",
    "    plot_list_series(\n",
    "        lst_series, \n",
    "        device_label, \n",
    "        wide_figure_size, \n",
    "        draw_markers=draw_markers\n",
    "    )\n",
    "\n",
    "def plot_df(df, device_label, wide_figure_size, draw_markers=False):\n",
    "    lst_series = [df[s] for s in df.columns]\n",
    "    plot_list_series(lst_series, device_label, wide_figure_size, draw_markers=draw_markers)\n",
    "\n",
    "def plot_confidence_intervals(df_bl, df_s, confidence_interval, title, figsize, x=None, y=None, hue=None, hue_order=None, individual=False, label_style=False):\n",
    "    label_bl = None\n",
    "    label_st = None\n",
    "\n",
    "    if (label_style is True):\n",
    "        label_bl = 'Línea base'\n",
    "        label_st = 'Periodo de estudio'\n",
    "    \n",
    "    if (individual is True):\n",
    "        plt.figure()\n",
    "\n",
    "    sns.lineplot(\n",
    "        x=x,\n",
    "        y=y,\n",
    "        hue=hue,\n",
    "        hue_order=hue_order,\n",
    "        data=df_bl.reset_index(), \n",
    "        ci=confidence_interval,\n",
    "        estimator=np.mean,\n",
    "        # palette=\"flare\",\n",
    "        label=label_bl\n",
    "    )\n",
    "\n",
    "    sns.lineplot(\n",
    "        x=x,\n",
    "        y=y,\n",
    "        hue=hue,\n",
    "        hue_order=hue_order,\n",
    "        linestyle=\"dashed\",\n",
    "        data=df_s.reset_index(), \n",
    "        ci=None,\n",
    "        legend=False,\n",
    "        estimator=np.mean,\n",
    "        # palette=\"flare\",\n",
    "        label=label_st\n",
    "    )\n",
    "    plt.title(title + f\" - Intervalo de confianza: {confidence_interval}%\")\n",
    "    plt.xlabel('Hora del día')\n",
    "    plt.ylabel('Consumo horario [kWh]')\n",
    "    plt.legend()\n",
    "    if (individual is True):\n",
    "        plt.show()\n",
    "    # plt.savefig('fig3/'+str(title)+'.png', dpi=300)\n",
    "\n",
    "    # plt.savefig(f'figuras/{title}.png', dpi=300)\n",
    "    # sns.set(rc={'figure.figsize':figsize})\n",
    "\n",
    "def run_pareto_analysis(df):\n",
    "    # ea-total must be dropped from pareto\n",
    "    is_target_var = (\n",
    "        (df['variable']!=ACTIVE_ENERGY_LABEL)\n",
    "        & (df['variable'].isin(Energy_VAR_LABELS))\n",
    "    )\n",
    "    df_pareto = df[is_target_var].copy()\n",
    "\n",
    "    s_pareto = df_pareto['value'].groupby(df_pareto['variable']).sum()\n",
    "    s_pareto.sort_values(ascending=False, inplace=True)\n",
    "\n",
    "    # Identify main variables\n",
    "    s_delta = s_pareto / s_pareto.sum()*100\n",
    "    elbow = s_delta.diff().astype(float).idxmin()\n",
    "    idx = s_delta.index.get_loc(elbow)\n",
    "    lst_main_labels = list(s_delta.iloc[:idx+1].index)\n",
    "\n",
    "    # make hue order before changing names\n",
    "    hue_order = make_hue_order(s_delta, lst_main_labels, new_label='ea-otros')\n",
    "\n",
    "    # The analysis is needed for power variables\n",
    "\n",
    "    return s_pareto\n",
    "\n",
    "\n",
    "def plot_pareto(series):\n",
    "    s_2 = series.cumsum() / series.sum()*100\n",
    "\n",
    "    fig, ax1 = plt.subplots()\n",
    "    ax2 = ax1.twinx()\n",
    "\n",
    "    ax1.bar(series.index, series, color=\"C0\")\n",
    "\n",
    "    ax2.plot(series.index, s_2, color=\"C1\", marker=\"D\", ms=7)\n",
    "    ax2.yaxis.set_major_formatter(PercentFormatter())\n",
    "    ax2.set_ylim(ymin=0)\n",
    "\n",
    "    ax1.tick_params(axis=\"y\", colors=\"C0\")\n",
    "    ax2.tick_params(axis=\"y\", colors=\"C1\")\n",
    "    plt.show()\n",
    "\n",
    "def make_hue_order(s_delta, lst_main_labels, new_label=None):\n",
    "    is_main_var = s_delta.index.isin(lst_main_labels)\n",
    "    s_sorter = s_delta[is_main_var]\n",
    "    s_sorter[new_label] = s_delta[~is_main_var].sum()\n",
    "    s_sorter.sort_values(ascending=False, inplace=True)\n",
    "    return list(s_sorter.index)\n",
    "\n",
    "def assemble_aggregator(lst_non_value_cols, value_method, bulk_method):\n",
    "    # make sure keys start with 'value'\n",
    "    lst_non_value_cols.remove('value')\n",
    "\n",
    "    lst_keys = ['value']\n",
    "    lst_keys.extend(lst_non_value_cols)\n",
    "\n",
    "    # assign value_method to value, bulk_method to rest\n",
    "    lst_values = [value_method]\n",
    "    lst_values.extend([bulk_method]*len(lst_non_value_cols))\n",
    "\n",
    "    return dict(zip(lst_keys, lst_values))\n",
    "\n",
    "\n",
    "def subset_group_by(df, agg_func=None, subset=None, group_by=None):\n",
    "    lst_df = []\n",
    "    for label in df[subset].unique():\n",
    "        is_selection = (df[subset]==label)\n",
    "        df_temp = df[is_selection].groupby(group_by).agg(agg_func)\n",
    "        df_temp.drop(columns=group_by, inplace=True)\n",
    "        lst_df.append(df_temp)\n",
    "\n",
    "    return pd.concat(lst_df, ignore_index=False)\n",
    "\n",
    "\n",
    "def subset_resample(df, agg_func=None, subset=None, freq=None):\n",
    "    lst_df = []\n",
    "    for label in df[subset].unique():\n",
    "        is_selection = (df[subset]==label)\n",
    "        df_temp = df[is_selection].resample(freq).agg(agg_func)\n",
    "        lst_df.append(df_temp)\n",
    "\n",
    "    return pd.concat(lst_df, ignore_index=False).dropna(how='all')\n",
    "\n",
    "\n",
    "def subset_drop_first_n_rows(df, subset=None, n_rows=None):\n",
    "    lst_df = []\n",
    "    for label in df[subset].unique():\n",
    "        is_selection = (df[subset]==label)\n",
    "        df_temp = df[is_selection]\n",
    "        lst_df.append(df_temp.iloc[n_rows:, :])\n",
    "\n",
    "    return pd.concat(lst_df, ignore_index=False)\n",
    "\n",
    "def lump_secondary_variables(df, lst_main_labels, new_variable_label, dct_agg, by=None):\n",
    "    \"\"\"\"\n",
    "    To avoid using the term \"grouped\" which might imply a groupby operation.\n",
    "    Lumped variables are termed \"others\" and represent a smaller quatity than main variables.\n",
    "    \"\"\"\n",
    "    # TODO: use by= to loop through devices\n",
    "    # replace secondary variable labels\n",
    "    is_main_var = df['variable'].isin(lst_main_labels)\n",
    "    df.loc[~is_main_var, 'variable'] = new_variable_label\n",
    "\n",
    "    lst_df = []\n",
    "    for variable in df['variable'].unique():\n",
    "        df_sel = df[df['variable']==variable]\n",
    "        \n",
    "        # index must be unique per variable\n",
    "        df_output = df_sel.groupby(df_sel.index).agg(dct_agg)\n",
    "        lst_df.append(df_output)\n",
    "\n",
    "    return pd.concat(lst_df, ignore_index=False)\n",
    "\n",
    "def find_consumption_delta_per_variable(df_bl, df_st, merge_on=None):\n",
    "    \"\"\"\n",
    "    Negative means a decrease in energy consumption\n",
    "    \"\"\"\n",
    "    df = pd.merge(df_bl, df_st, on=merge_on)\n",
    "    df['delta'] = df['value_y'] - df['value_x']\n",
    "    return df[['delta', 'variable']]\n",
    "\n",
    "def subplots_stack(df1, df2, figsize):\n",
    "    df1_wide = df1.pivot(index=None, columns='variable', values='value')\n",
    "    df2_wide = df2.pivot(index=None, columns='variable', values='value')\n",
    "\n",
    "    f, (ax, bx) = plt.subplots(1,2,sharey=True) # like mine\n",
    "    ax.stackplot(df1_wide.index, df1_wide.T, labels=list(df1_wide.columns))\n",
    "    bx.stackplot(df2_wide.index, df2_wide.T, labels=list(df2_wide.columns))\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Cleaning parameters\n",
    "clean_data = True\n",
    "validate_cleaning = False\n",
    "\n",
    "SHORT_WINDOW = 4 #'5h'\n",
    "SHORT_CONFIDENCE_INTERVAL = 97.5\n",
    "\n",
    "LONG_WINDOW = 480 # '5D'\n",
    "LONG_CONFIDENCE_INTERVAL = 99\n",
    "\n",
    "SMOOTHING_METHOD = 'mean'\n",
    "SMOOTHING_WINDOW = 4 # '180min'\n",
    "\n",
    "# Plotting parameters\n",
    "confidence_interval = 95\n",
    "\n",
    "# Ubidots API\n",
    "API_URL = 'https://industrial.api.ubidots.com/api/v1.6/devices/'\n",
    "LST_VAR_FIELDS = [\"value.value\", \"variable.id\", \"device.label\", \"device.name\", \"timestamp\"]\n",
    "LST_HEADERS = ['value', 'variable', 'device', 'device_name', 'timestamp']\n",
    "\n",
    "# Date and Time\n",
    "DATETIME_FORMAT = \"%Y-%m-%dT%H:%M:%S\"\n",
    "DATE_FORMAT = \"%Y-%m-%d\"\n",
    "LOCAL_TIMEZONE = 'America/Bogota'\n",
    "\n",
    "\n",
    "wide_figure_size = (21,7)\n",
    "\n",
    "# General parameters\n",
    "ALLOWED_DATE_OVERLAP = 0\n",
    "DAYS_PER_MONTH = 365.25/12\n",
    "\n",
    "# Ubidots data parameters\n",
    "agg_frequency = '1H'\n",
    "\n",
    "dct_dow = {\n",
    "    0:'lunes',\n",
    "    1:'martes',\n",
    "    2:'miércoles',\n",
    "    3:'jueves',\n",
    "    4:'viernes',\n",
    "    5:'sábado',\n",
    "    6:'domingo',\n",
    "} \n",
    "\n",
    "plt.figure()\n",
    "plt.show()\n",
    "sns.set(rc={'figure.figsize': wide_figure_size})\n",
    "plt.close()\n",
    "\n",
    "save_figures = False\n",
    "show_optional_figures = False\n",
    "cop_per_kwh = 692.29\n",
    "\n",
    "# Specify the date interval to fetch data from\n",
    "# the format must be: 'YYYY-MM-DD'\n",
    "BASELINE_DATE_INTERVAL = cfg.BASELINE_DATE_INTERVAL\n",
    "STUDY_DATE_INTERVAL = cfg.STUDY_DATE_INTERVAL\n",
    "\n",
    "DATE_INTERVALS_TO_DISCARD = [\n",
    "#   ['2022-05-11', '2022-05-22']\n",
    "]\n",
    "\n",
    "NOCTURNE = [0, 1, 2, 3, 4, 5, 19, 20, 21, 22, 23]\n",
    "\n",
    "\n",
    "ACTIVE_ENERGY_LABEL = 'consumo-activa-total'\n",
    "ACTIVE_POWER_LABEL = 'pa-total'\n",
    "POWER_NON_MAIN_LABEL = 'pa-otros' # not used for Molinos\n",
    "\n",
    "Energy_VAR_LABELS = ('aa-consumo-activa', 'ilu-consumo-activa')\n",
    "Power_VAR_LABELS = ('aa-potencia-activa', 'ilu-potencia-activa')\n",
    "\n",
    "SUB_STR = ('aa', 'front', 'ilu', 'consumo-activa', 'consumo-energia-reactiva')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle(PICKLED_DATA_FILEPATH)\n",
    "df = df.query(\"device_name == @DEVICE_NAME\")\n",
    "show_response_contents(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "df = post_process_data(df)\n",
    "print(df[\"variable\"].unique())\n",
    "\n",
    "df_bl = df.loc[BASELINE_DATE_INTERVAL['start']:BASELINE_DATE_INTERVAL['end']]\n",
    "df_st = df.loc[STUDY_DATE_INTERVAL['start']:STUDY_DATE_INTERVAL['end']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "cargas = df_st[df_st[\"variable\"].isin(Energy_VAR_LABELS)].copy()\n",
    "front = df_st[df_st[\"variable\"].isin(['front-consumo-activa'])].copy()\n",
    "front_pot = df_st[df_st[\"variable\"].isin(['front-potencia-activa'])].copy()\n",
    "front_reactiva = df_st[df_st[\"variable\"].isin(['consumo-energia-reactiva-total'])].copy()\n",
    "cargas_pot = df_st[df_st[\"variable\"].isin(Power_VAR_LABELS)].copy()\n",
    "cargas_nocturne = cargas[cargas[\"hour\"].isin(NOCTURNE)].copy()\n",
    "\n",
    "\n",
    "\n",
    "# past_months = df_bl[df_bl[\"variable\"] == 'front-consumo-activa'].groupby(by=[\"variable\"]).resample('1M').sum().round(2).reset_index().set_index('datetime')\n",
    "# past_months = apply_datetime_transformations(past_months)\n",
    "\n",
    "# past_hour = df_bl[df_bl[\"variable\"] == 'front-consumo-activa'].groupby(by=[\"variable\"]).resample('1h').sum().round(2).reset_index().set_index('datetime')\n",
    "# past_hour = apply_datetime_transformations(past_hour)\n",
    "\n",
    "cargas_month = cargas.groupby(by=[\"variable\"]).resample('1M').sum().round(2).reset_index().set_index('datetime')\n",
    "cargas_month = apply_datetime_transformations(cargas_month)\n",
    "\n",
    "cargas_day = cargas.groupby(by=[\"variable\"]).resample('1D').sum().round(2).reset_index().set_index('datetime')\n",
    "cargas_day = apply_datetime_transformations(cargas_day)\n",
    "\n",
    "cargas_hour = cargas.groupby(by=[\"variable\"]).resample('1h').sum().round(2).reset_index().set_index('datetime')\n",
    "cargas_hour = apply_datetime_transformations(cargas_hour)\n",
    "\n",
    "cargas_pot_hour = cargas_pot.groupby(by=[\"variable\"]).resample('1h').sum().round(2).reset_index().set_index('datetime')\n",
    "cargas_pot_hour = apply_datetime_transformations(cargas_pot_hour)\n",
    "\n",
    "front_pot_hour = front_pot.groupby(by=[\"variable\"]).resample('1h').sum().round(2).reset_index().set_index('datetime')\n",
    "front_pot_hour = apply_datetime_transformations(front_pot)\n",
    "\n",
    "front_hour = front.groupby(by=[\"variable\"]).resample('1h').sum().round(2).reset_index().set_index('datetime')\n",
    "front_hour = apply_datetime_transformations(front_hour)\n",
    "\n",
    "front_month = front.groupby(by=[\"variable\"]).resample('1M').sum().round(2).reset_index().set_index('datetime')\n",
    "front_month = apply_datetime_transformations(front_month)\n",
    "\n",
    "front_day = front.groupby(by=[\"variable\"]).resample('1D').sum().round(2).reset_index().set_index('datetime')\n",
    "front_day = apply_datetime_transformations(front_day)\n",
    "\n",
    "front_reactiva_hour = front_reactiva.groupby(by=[\"variable\"]).resample('1h').sum().round(2).reset_index().set_index('datetime')\n",
    "front_reactiva_hour = apply_datetime_transformations(front_reactiva_hour)\n",
    "\n",
    "Cargas_Nocturne_day = cargas_nocturne.groupby(by=[\"variable\"]).resample('1D').sum().round(2).reset_index().set_index('datetime')\n",
    "Cargas_Nocturne_day = apply_datetime_transformations(Cargas_Nocturne_day)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "consumo_sede = front_month.iloc[-1][\"value\"]\n",
    "# dif_mes_anterior =front_month.iloc[-1][\"value\"] - past_months.iloc[-1][\"value\"]\n",
    "print(f\"El consumo de energía durante el último mes fue: {round(consumo_sede,2)} kWh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "cargas_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "cargas_grouped = cargas_month['value'].sum()\n",
    "consumo_otros =  consumo_sede -cargas_grouped \n",
    "consumo_otros\n",
    "\n",
    "piechart_df = cargas_month[[\"value\",\"variable\"]].copy()\n",
    "piechart_df.loc[-1] = [consumo_otros, \"Otros\"]\n",
    "piechart_df.reset_index(inplace=True, drop=True)\n",
    "piechart_df['value'] = piechart_df['value'].round(decimals = 2)\n",
    "\n",
    "\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.pie(piechart_df[\"value\"],  labels=piechart_df[\"variable\"], autopct='%1.1f%%',\n",
    "        shadow=True, startangle=90)\n",
    "ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "plt.show()\n",
    "\n",
    "\n",
    "piechart_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "sns.barplot(x=\"day\", y=\"value\", data=front_day, color=\"#D5752D\")\n",
    "plt.title(\"Consumo diario de energía activa (kWh) en el último mes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "sns.barplot(x=\"day\", y=\"value\", hue=\"variable\", data=pd.concat([cargas_day, front_day]))\n",
    "plt.title(\"Consumo diario de energía activa (kWh) en el último mes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "df_study_datehour = front_hour.groupby('datetime').sum()\n",
    "df_study_datehour['hour'] = df_study_datehour.index.hour\n",
    "df_study_datehour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "# df_baseline_datehour = past_hour.groupby('datetime').sum()\n",
    "df_study_datehour = front_hour.groupby('datetime').sum()\n",
    "# df_baseline_datehour['hour'] = df_baseline_datehour.index.hour\n",
    "df_study_datehour['hour'] = df_study_datehour.index.hour\n",
    "\n",
    "device_name = df['device_name'][0]\n",
    "title = f\"{device_name} - Consumo total horario\"\n",
    "\n",
    "\n",
    "# sns.lineplot(\n",
    "#     x='hour',\n",
    "#     y='value',\n",
    "#     hue=None,\n",
    "#     data=df_baseline_datehour.reset_index(), \n",
    "#     ci=confidence_interval,\n",
    "#     estimator=np.mean,\n",
    "#     # palette=\"flare\",\n",
    "#     label=\"Consumo meses pasados\"\n",
    "# )\n",
    "sns.lineplot(\n",
    "    x='hour',\n",
    "    y='value',\n",
    "    hue=None,\n",
    "    linestyle=\"dashed\",\n",
    "    data=df_study_datehour.reset_index(), \n",
    "    ci=None,\n",
    "    legend=False,\n",
    "    estimator=np.mean,\n",
    "    # palette=\"flare\",\n",
    "    label=\"Mes actual\"\n",
    ")\n",
    "plt.title(title + f\" - Intervalo de confianza: {confidence_interval}%\")\n",
    "plt.xlabel('Hora del día')\n",
    "plt.ylabel('Consumo horario [kWh]')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "for day in dct_dow.values():\n",
    "    # df_plot_bl = past_hour[(past_hour['dow']==day)].copy()\n",
    "    df_plot_s = front_hour[(front_hour['dow']==day)].copy()\n",
    "    device_name = df['device_name'][0]\n",
    "    title = f\"{device_name} - Consumo horario para el día {day}\"\n",
    "    \n",
    "    \n",
    "    # sns.lineplot(\n",
    "    #     x='hour',\n",
    "    #     y='value',\n",
    "    #     hue=None,\n",
    "    #     data=df_plot_bl.reset_index(), \n",
    "    #     ci=confidence_interval,\n",
    "    #     estimator=np.median,\n",
    "    #     # palette=\"flare\",\n",
    "    #     label=\"Consumo meses pasados\"\n",
    "    # )\n",
    "    sns.lineplot(\n",
    "        x='hour',\n",
    "        y='value',\n",
    "        hue=None,\n",
    "        linestyle=\"dashed\",\n",
    "        data=df_plot_s.reset_index(), \n",
    "        ci=None,\n",
    "        legend=False,\n",
    "        estimator=np.mean,\n",
    "        # palette=\"flare\",\n",
    "        label=\"Mes actual\"\n",
    "    )\n",
    "    plt.title(title + f\" - Intervalo de confianza: {confidence_interval}%\")\n",
    "    plt.xlabel('Hora del día')\n",
    "    plt.ylabel('Consumo horario [kWh]')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "b = front_hour[[\"day\",\"hour\", \"value\"]]\n",
    "matrix = b.pivot(\"day\", \"hour\", \"value\")\n",
    "sns.heatmap(matrix, annot=True, cmap=\"YlOrRd\", linewidths=.5)\n",
    "plt.title(\"Matriz de consumo horario (frontera) [kWh] en el último mes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "c = cargas_hour[[\"day\",\"hour\", \"value\"]].groupby(by=[\"day\",\"hour\"]).sum().reset_index()\n",
    "matrix_c = c.pivot(\"day\", \"hour\", \"value\")\n",
    "sns.heatmap(matrix_c, annot=True, cmap=\"YlOrRd\", linewidths=.5)\n",
    "plt.title(\"Matriz de consumo horario (Cargas) [kWh] en el último mes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "r = front_reactiva_hour[[\"day\",\"hour\", \"value\"]].groupby(by=[\"day\",\"hour\"]).sum().reset_index()\n",
    "matrix_r = r.pivot(\"day\", \"hour\", \"value\")\n",
    "sns.heatmap(matrix_r, annot=True, cmap=\"YlOrRd\", linewidths=.5)\n",
    "plt.title(\"Matriz de consumo reactiva horario (Front) [kVArh] en el último mes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "cargas_hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "sns.lineplot(\n",
    "        x='datetime',\n",
    "        y='value',\n",
    "        linestyle=\"dotted\",\n",
    "        data=cargas_hour[cargas_hour[\"variable\"]==\"aa-consumo-activa\"], \n",
    "        ci=None,\n",
    "        legend=False,\n",
    "        # palette=\"flare\",\n",
    "        label=\" aa [kWh]\"\n",
    "    )\n",
    "\n",
    "sns.lineplot(\n",
    "        x='datetime',\n",
    "        y='value',\n",
    "        linestyle=\"dashed\",\n",
    "        data=cargas_hour[cargas_hour[\"variable\"]==\"ilu-consumo-activa\"], \n",
    "        ci=None,\n",
    "        legend=False,\n",
    "        # palette=\"flare\",\n",
    "        label=\" ilu [kWh]\"\n",
    "    )\n",
    "\n",
    "sns.lineplot(\n",
    "        x='datetime',\n",
    "        y='value',\n",
    "        linestyle=\"solid\",\n",
    "        data=front_hour[front_hour[\"variable\"]==\"front-consumo-activa\"], \n",
    "        ci=None,\n",
    "        legend=False,\n",
    "        # palette=\"flare\",\n",
    "        label=\" front [kWh]\"\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "plt.title(\"Curva de potencia de los circuitos medidos (kWh) en el último mes\")\n",
    "plt.xlabel('Fecha')\n",
    "plt.ylabel('Consumo horario')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "sns.barplot(\n",
    "        x='day',\n",
    "        y='value',\n",
    "        hue=None,\n",
    "        linestyle=\"dashed\",\n",
    "        data=Cargas_Nocturne_day, \n",
    "        ci=None,\n",
    "        color=\"Orange\",\n",
    "    )\n",
    "plt.title(\"Consumo nocturno de cargas monitoreadas (kWh) en el último mes\")\n",
    "plt.xlabel('Fecha')\n",
    "plt.ylabel('Consumo [kWh]')\n",
    "\n",
    "\n",
    "consumo_nocturno = round(Cargas_Nocturne_day[\"value\"].sum(),2)\n",
    "\n",
    "print(\"Durante el mes pasado se consumió un total de: \", consumo_nocturno, \"kWh fuera del horario establecido\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "3e8543986d11a7095dd1a708864c19914edc7f3db3f09d9443f7bd38b5a3c994"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
